{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tnm_k3t4EkCr",
        "outputId": "b8ede72c-f898-4785-98b2-0a1274294bbe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Agent's path to reach the goal:\n",
            "Step 1: (0, 0)\n",
            "Step 2: (0, 1)\n",
            "Step 3: (0, 2)\n",
            "Step 4: (0, 3)\n",
            "Step 5: (0, 4)\n",
            "Step 6: (1, 4)\n",
            "Step 7: (2, 4)\n",
            "Step 8: (3, 4)\n",
            "Step 9: (4, 4)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "class GridWorld:\n",
        "    def __init__(self, size=5):\n",
        "        self.size = size\n",
        "        self.grid = np.zeros((size, size))  # Initialize grid\n",
        "        self.agent_position = (0, 0)  # Agent starts at top-left\n",
        "        self.goal_position = (size-1, size-1)  # Goal is at bottom-right\n",
        "        self.obstacles = [(1, 1), (2, 2), (3, 3)]  # Obstacle positions\n",
        "        self.actions = [(0, 1), (1, 0), (0, -1), (-1, 0)]  # Right, Down, Left, Up\n",
        "        self.q_table = np.zeros((size, size, len(self.actions)))  # Q-table\n",
        "\n",
        "    def get_valid_actions(self, position):\n",
        "        valid_actions = []\n",
        "        for action in self.actions:\n",
        "            new_position = (position[0] + action[0], position[1] + action[1])\n",
        "            if 0 <= new_position[0] < self.size and 0 <= new_position[1] < self.size \\\n",
        "                    and new_position not in self.obstacles:\n",
        "                valid_actions.append(action)\n",
        "        return valid_actions\n",
        "\n",
        "    def take_action(self, position, action):\n",
        "        new_position = (position[0] + action[0], position[1] + action[1])\n",
        "        if new_position == self.goal_position:\n",
        "            reward = 1  # Reached goal\n",
        "        elif new_position in self.obstacles:\n",
        "            reward = -1  # Hit obstacle\n",
        "            new_position = position  # Stay in the same position\n",
        "        else:\n",
        "            reward = 0  # Normal movement\n",
        "        return new_position, reward\n",
        "\n",
        "    def train(self, episodes=100, alpha=0.1, gamma=0.9, epsilon=0.1):\n",
        "        for _ in range(episodes):\n",
        "            state = self.agent_position\n",
        "            while state != self.goal_position:\n",
        "                valid_actions = self.get_valid_actions(state)\n",
        "                if np.random.uniform(0, 1) < epsilon:\n",
        "                    action = valid_actions[np.random.randint(0, len(valid_actions))]\n",
        "                else:\n",
        "                    action = max(valid_actions, key=lambda a: self.q_table[state[0], state[1], self.actions.index(a)])\n",
        "                next_state, reward = self.take_action(state, action)\n",
        "                self.q_table[state[0], state[1], self.actions.index(action)] += \\\n",
        "                    alpha * (reward + gamma * np.max(self.q_table[next_state[0], next_state[1], :]) -\n",
        "                             self.q_table[state[0], state[1], self.actions.index(action)])\n",
        "                state = next_state\n",
        "            self.agent_position = (0, 0)  # Reset agent position\n",
        "\n",
        "    def test(self):\n",
        "        state = self.agent_position\n",
        "        path = [state]\n",
        "        while state != self.goal_position:\n",
        "            action = max(self.actions, key=lambda a: self.q_table[state[0], state[1], self.actions.index(a)])\n",
        "            next_state, _ = self.take_action(state, action)\n",
        "            path.append(next_state)\n",
        "            state = next_state\n",
        "        return path\n",
        "\n",
        "# Initialize and train the gridworld\n",
        "grid_world = GridWorld(size=5)\n",
        "grid_world.train(episodes=1000)\n",
        "\n",
        "# Test the trained model\n",
        "path = grid_world.test()\n",
        "\n",
        "print(\"Agent's path to reach the goal:\")\n",
        "for step, position in enumerate(path):\n",
        "    print(f\"Step {step+1}: {position}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3OVZg1UT33dw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}